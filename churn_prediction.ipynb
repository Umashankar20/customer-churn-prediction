# Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Step 1: Data Collection
# Assume we have a dataset 'customer_data.csv'
data = pd.read_csv('customer_data.csv')

# Step 2: Data Preprocessing
# Checking for missing values
print(data.isnull().sum())

# Filling missing values or dropping rows with missing values
data = data.dropna()

# Encoding categorical variables
data = pd.get_dummies(data, drop_first=True)

# Step 3: Exploratory Data Analysis (EDA)
# Distribution of churn
sns.countplot(x='Churn', data=data)
plt.show()

# Correlation matrix
corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True)
plt.show()

# Step 4: Feature Engineering
# Splitting the dataset into features and target variable
X = data.drop('Churn', axis=1)
y = data['Churn']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Step 5: Model Building and Evaluation
# Initializing the RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Training the model
model.fit(X_train, y_train)

# Making predictions
y_pred = model.predict(X_test)

# Evaluating the model
print('Accuracy:', accuracy_score(y_test, y_pred))
print('Confusion Matrix:\n', confusion_matrix(y_test, y_pred))
print('Classification Report:\n', classification_report(y_test, y_pred))

# Step 6: Model Deployment (Optional, showcasing as a concept)
import joblib
joblib.dump(model, 'churn_model.pkl')

# Loading the model (for demonstration purposes)
loaded_model = joblib.load('churn_model.pkl')
sample_data = X_test[0].reshape(1, -1)
print('Sample prediction:', loaded_model.predict(sample_data))

# Step 7: Visualization and Reporting
# Feature importance
feature_importances = pd.DataFrame(model.feature_importances_,
                                   index = X.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)
sns.barplot(x=feature_importances['importance'], y=feature_importances.index)
plt.show()

# Saving the EDA and feature importance plots
sns.countplot(x='Churn', data=data).figure.savefig('churn_distribution.png')
sns.heatmap(corr_matrix, annot=True).figure.savefig('correlation_matrix.png')
sns.barplot(x=feature_importances['importance'], y=feature_importances.index).figure.savefig('feature_importance.png')

# Summary report (can be extended as a PDF report or a blog post)
report = f"""
# Customer Churn Prediction Project

## Introduction
Customer churn prediction is critical for subscription-based services to retain customers and sustain revenue. This project aims to build a machine learning model to predict churn.

## Data Preprocessing
- Missing values handled
- Categorical variables encoded

## Exploratory Data Analysis
- Churn distribution plot
- Correlation matrix

## Model Building
- RandomForestClassifier used
- Accuracy: {accuracy_score(y_test, y_pred)}
- Confusion Matrix: {confusion_matrix(y_test, y_pred)}
- Classification Report: {classification_report(y_test, y_pred)}

## Feature Importance
Top features contributing to the prediction:

{feature_importances.head()}

## Conclusion
The model provides a reliable way to predict customer churn, aiding in strategic decision-making for customer retention.
"""

# Save the report
with open('churn_prediction_report.txt', 'w') as file:
    file.write(report)
